When considering the interaction between any given system and a sufficiently complex environment, inherently the relationship between operation and safety is dependent on the degree in which uncertainty can be mapped.

The railway environment is notably complex and currently burdened with conflicting terminologies and fragmented standards. Multiple classification schemes, GoA[[12]](cite:12), FAO[[13]](cite:13), ETCS[[16]](cite:16) and others [[19]](cite:19) all attempt to define the same operational landscape, yet none provide the holistic clarity or flexibility required. To this degree, when classifying technology, or system requirements, SACRED acts classification agnostic, treating system envrionemnts as uncertainty, rather than given parts of a classification schema.

We base this agnostic understanding of uncertainty on Lovell's Manifestation of Uncertainty [[4]](cite:4). Lovell argues that within any scenario where knowledge is shared, one participant will always have authority over the other. When participant A has authority over B, B must assume that A has all of their knowledge and more, if not, it is the duty of B to inform A of the gap. This creates a sort of Asymmetric relationship between A and B, B inherently must assume that A is all knowing, yet at the same time, acknowledge that it is possible for A to make a mistake. When considering safety, we remember the words of Richard Nixon:

*"Trust but Verify"*

Burton [[10]](cite:10) expands on this idea, suggesting that inside any autonomous system, decisions are ultimately made from evidence given by a colletion agent, be this a camera, a third party or a dataset. This information collected is inherently an abstract of the real world, with certain amounts of data being deemed irrelevant, [obstructed](cite:e1) or simply not in scope.


<img src="/Images/manifestations.png"
     alt="Manifestations of Uncertainty Represation Figure"
     style="width:800px;max-width:100%;height:auto;border-radius:8px;" />

This results in uncertainty manifesting as a series of abstractions. The entirety of the world is encompassed by our model of the world, the [ODM](ref:b), the representation of the world is then divided into that which is relevant for the system and that which is not, which we do through our [ecoego assignment](ref:d5) and finally, this evidence is passed to the system and depending on the level of complexity of the system, a decision is ultimately made.

Talk about stopgaps and how a system can verify it's own evidence and see stuff is lacking iykyk