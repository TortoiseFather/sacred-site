When considering the interaction between any given system and a sufficiently complex environment, inherently the relationship between operation and safety is dependent on the degree in which uncertainty can be mapped.

The railway environment is notably complex and currently burdened with conflicting terminologies and fragmented standards. Multiple classification schemes, GoA[[12]](cite:12), FAO[[13]](cite:13), ETCS[[16]](cite:16) and others [[19]](cite:19) all attempt to define the same operational landscape, yet none provide the holistic clarity or flexibility required. To this degree, when classifying technology, or system requirements, SACRED acts classification agnostic, treating system envrionemnts as uncertainty, rather than given parts of a classification schema.

We base this agnostic understanding of uncertainty on Lovell's Manifestation of Uncertainty [[4]](cite:4). Lovell argues that within any scenario where knowledge is shared, one participant will always have authority over the other. When participant A has authority over B, B must assume that A has all of their knowledge and more, if not, it is the duty of B to inform A of the gap. This creates a sort of Asymmetric relationship between A and B, B inherently must assume that A is all knowing, yet at the same time, acknowledge that it is possible for A to make a mistake.

Burton [[10]](cite:10) expands on this idea, suggesting that inside any autonomous system, decisions are ultimately made from evidence given by a colletion agent, be this a camera, a third party or a dataset. This information collected is inherently an abstract of the real world, with certain amounts of data being deemed irrelevant, [obstructed](cite:e1) or simply not in scope.


<img src="/Images/manifestations.png"
     alt="Manifestations of Uncertainty Represation Figure"
     style="width:800px;max-width:100%;height:auto;border-radius:8px;" />

This results in uncertainty manifesting as a series of abstractions. The entirety of the world is encompassed by our model of the world, the [ODM](ref:b), the representation of the world is then divided into that which is relevant for the system and that which is not, which we do through our [ecoego assignment](ref:d5) and finally, this evidence is passed to the system and depending on the level of complexity of the system, a decision is ultimately made. However, when considering safety, we remember the words of Richard Nixon:

*"Trust but Verify"*

Thinking back to Lovell's example, we consider the relationship within the railway ecosystem, the cab inherently must believe that the information it has been given from it's signal engineer is true, however, we understand that it is possible for Signal Engineers to make mistakes. In the book 'People and Rail systems', Luther, the author of chapter 7 'Understanding Driver Route knowledge' [[21]](cite:21) describes the information a driver needs in order to navigate a route in real time. This, in turn, implies that the navigation of a route is more than reaction and more information is needed than what the driver can perceive in a given moment. The driver must understand the environment around them and from that, understand what the future track will look like.

Luther adapts the work of Endsley et al [[22]](cite:22), a model monitoring situational awareness, to argue that when making a decision, a driver first processes the command given by the signal operator then visualises the impact that command would have on the immediate operation of the train, abstracting away any irrelevant details, before internally analysing the impact of what remains, relying upon route knowledge and understanding of mitigation of possible changes. A similar approach to the triple abstraction methodology proposed by Lovell. That is to say, when abstracting a hazard, it is done from both the ecosystem perspective in order to shape the ODM and from an ego-vehicle perspective, determining hazard verification through [Hazard Compensation.](/#/steps/2-hazard-identification/examples/2.4)